# -*- coding: utf-8 -*-
"""Anote_AI_Challenge_Llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EANMDSuWslWSe-Xip-_5Ik-osh8Ts6Fx
"""

!pip install -qU bitsandbytes datasets accelerate loralib transformers peft

from google.colab import drive
drive.mount('/content/drive')

!pip install trl -U -q

import pandas as pd
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM

import pandas as pd
import json
import torch
from sklearn.model_selection import train_test_split

csv_file_path = "/content/drive/MyDrive/financebench_sample_150.csv"
csv_data = pd.read_csv(csv_file_path)
jsonl_file_path = "/content/drive/MyDrive/rag_instruct_benchmark_tester.jsonl"

jsonl_data = []
with open(jsonl_file_path, 'r') as jsonl_file:
    for line in jsonl_file:
        json_data = json.loads(line)
        jsonl_data.append(json_data)

# Create DataFrame from JSONL data
jsonl_df = pd.DataFrame(jsonl_data)

data = pd.DataFrame(columns=['question', 'answer', 'context'])

data = pd.concat([data, csv_data[['question', 'answer', 'evidence_text']].rename(columns={'evidence_text': 'context'})], ignore_index=True)
data = pd.concat([data, jsonl_df[['query', 'answer', 'context']].rename(columns={'query': 'question'})], ignore_index=True)

data.dropna(inplace=True)

data.head()

len(data)

data.to_csv("finqa_dataset_merged.csv", index=False)

df= pd.read_csv("/content/drive/MyDrive/finqa_dataset_merged.csv")

df.head()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

train_df

train_data = []
for index, row in train_df.iterrows():
    train_data.append({
        'question': row['question'],
        'context': row['context'],
        'answer': row['answer']
    })

val_data = []
for index, row in val_df.iterrows():
    val_data.append({
        'question': row['question'],
        'context': row['context'],
        'answer': row['answer']
    })

len(train_data)

!pip install huggingface-hub

!huggingface-cli login

!huggingface-cli whoami

from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig
import torch
import os

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    use_cache=False,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

print(base_model)

def count_trainable_params(model):
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return trainable_params

count_trainable_params(base_model)

#implementing Low Rank Adaptation using peft
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    #target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

base_model = prepare_model_for_kbit_training(base_model)
model = get_peft_model(base_model, lora_config)
count_trainable_params(model)

print(model)

#function to generate QnA
def generate_prompt(example, return_response=True) -> str:
    prompt = f"[CONTEXT]: {example['context']} [QUESTION]: {example['question']}"

    if return_response:
        prompt += f" [ANSWER]: {example['answer']}"
    return prompt

train_data[0]

train_prompts = [generate_prompt({'question': d['question'], 'context': d['context'], 'answer': d['answer']}, return_response=True) for d in train_data]

len(train_prompts)

train_data[0]

train_prompts[0]

val_prompts = [generate_prompt({'question': d['question'], 'context': d['context'], 'answer': d['answer']}, return_response=True) for d in val_data]

val_prompts[0]

len(val_prompts)

#generate encodings using tokenizer
train_encodings = tokenizer(train_prompts, truncation=True, padding=True, max_length=2048, return_tensors="pt")
val_encodings = tokenizer(val_prompts, truncation=True, padding=True, max_length=2048, return_tensors="pt")

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="output",
    evaluation_strategy="steps",
    eval_steps=500,
    logging_strategy="steps",
    logging_steps=100,
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    weight_decay=0.01,
    warmup_steps=100,
    gradient_accumulation_steps=8,
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    report_to="none",
)

from trl import SFTTrainer

#using SFTTrainer because we have small labeled data for the task

from torch.utils.data import Dataset

class CustomTextDataset(Dataset):
    def __init__(self, encoded_data):
        self.encoded_data = encoded_data

    def __getitem__(self, index):
        sample = {key: torch.tensor(value[index]) for key, value in self.encoded_data.items()}
        return sample

    def __len__(self):
        return len(self.encoded_data['input_ids'])

train_dataset= CustomTextDataset(train_encodings)
val_dataset= CustomTextDataset(val_encodings)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    peft_config=lora_config,
    max_seq_length=2048,
    formatting_func=generate_prompt,
    tokenizer=tokenizer,
)

trainer.train()

trainer.save()

from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    training_args.output_dir,
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    load_in_4bit=True,
)
tokenizer = AutoTokenizer.from_pretrained(training_args.output_dir)

import json

def structure_json_data(json_text):
    def organize_element(data, indent_level=0):
        formatted_lines = []
        if isinstance(data, dict):
            for key, value in data.items():
                formatted_lines.append(" " * indent_level + f"{key}:")
                formatted_lines.extend(organize_element(value, indent_level + 2))
        elif isinstance(data, list):
            for idx, item in enumerate(data, start=1):
                formatted_lines.append(" " * indent_level + f"- Item {idx}:")
                formatted_lines.extend(organize_element(item, indent_level + 2))
        else:
            formatted_lines.append(" " * indent_level + str(data))
        return formatted_lines

    json_data = json.loads(json_text)
    structured_text = "\n".join(organize_element(json_data))
    return structured_text

def process_codefinqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You compute values from a given document context. To do so, first work through an answer step-by-step in natural language and then write a single bit of python code to compute the final solution. The python code cannot import external libraries or print any values. The implicit return value from the last statement will be provided as the answer. The answer should always be a single float value. Put the code in a markdown code block. (```python)"
    prompt = f"Context: {context} Question: {question} \n\n###\n\n"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n```python\n{answer}\n```"

def process_convfinqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Make sure the number in the brackets matches the requested units. Any response without a [[]] is invalid."
    prompt = f"Context: {context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_fincode_code(model, tokenizer, question, context):
    system_message = "You are a QA bot. You compute values to answer a given question. To do so, first work through an answer step-by-step in natural language and then write a single bit of python code to compute the final solution. The python code cannot import external libraries or print any values. The implicit return value from the last statement will be provided as the answer. The answer should always be a single float value. Put the code in a markdown code block. (```python)"
    prompt = f"Context: {context} Question: {question} \n\n###\n\n"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n```python\n{answer}\n```"

def process_finknow(model, tokenizer, question, context, options):
    system_message = "You are a multiple choice bot. You respond to the question by first thinking through the questions and the possible answers, and then respond with the final choice in as [[LETTER]]. For example, if the options are A, B and C and the answer is A, the final part of your message should be [[A]]. Do not include any other text within the [[]]"
    prompt = f"Context: {context} Question: {question} Options: {options}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_tatqa(model, tokenizer, question, context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Make sure the number in the brackets matches the requested units. Any response without a [[]] is invalid."
    prompt = f"Context: {context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

def process_codetatqa(model, tokenizer, question, json_context):
    system_message = "You are a document QA bot. You return values from a given document context. You respond to the question by first thinking through the question step-by-step, and then provide with the final solution as [[NUMBER]]. Do not include any other text within the [[]], it should be a single number that could be directly cast into a python float. Any response without a [[]] is invalid."
    formatted_context = format_json_context(json_context)
    prompt = f"Context: {formatted_context} Question: {question}"
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).input_ids.cuda()
    output = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.5)
    answer = tokenizer.decode(output[0], skip_special_tokens=True)
    return system_message + f"\n\n[[{answer}]]"

import pandas as pd

test_df = pd.read_excel('/content/drive/MyDrive/anote/TestingData GSheets.xlsx')

results = []

for index, row in test_df.iterrows():
    task_function_map = {
        'CodeFinQA': process_codefinqa,
        'ConvFinQA': process_convfinqa,
        'FinCode': process_fincode_code,
        'FinKnow': process_finknow,
        'CodeTAT-QA': process_codetatqa,
        'TAT-QA': process_tatqa
    }

    process_function = task_function_map.get(row['task'])

    if process_function:
        if row['task'] == 'FinKnow':
            answer = process_function(model, tokenizer, row['question'], row['context'], row['options'])
        elif row['task'] == 'CodeTAT-QA':
            answer = process_function(model, tokenizer, row['question'], row['context'])
        else:
            answer = process_function(model, tokenizer, row['question'], row['context'])
    else:
        answer = "Task not recognized"

    results.append({'id': row['id'], 'answer': answer})

results_df = pd.DataFrame(results)
results_df.to_csv("inference_results.csv", index=False)